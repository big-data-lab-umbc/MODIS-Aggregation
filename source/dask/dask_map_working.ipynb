{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import dask.delayed as delayed\n",
    "import time\n",
    "import math\n",
    "#import graphviz\n",
    "from netCDF4 import Dataset\n",
    "import os,datetime,sys,fnmatch\n",
    "import h5py\n",
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_filelist(loc_dir,prefix,unie,fileformat):\n",
    "    # Read the filelist in the specific directory\n",
    "    str = os.popen(\"ls \"+ loc_dir + prefix + unie + \"*.\"+fileformat).read()\n",
    "    fname = np.array(str.split(\"\\n\"))\n",
    "    fname = np.delete(fname,len(fname)-1)\n",
    "    return fname\n",
    "\n",
    "def read_MODIS(fname1,fname2,verbose=False): # READ THE HDF FILE\n",
    "    # Read the cloud mask from MYD06_L2 product') \n",
    "    ncfile=Dataset(fname1,'r')\n",
    "    CM1km = np.array(ncfile.variables['Cloud_Mask_1km'])\n",
    "    CM   = (np.array(CM1km[:,:,0],dtype='byte') & 0b00000110) >>1\n",
    "    ncfile.close()\n",
    "    \n",
    "    ncfile=Dataset(fname2,'r')\n",
    "    lat  = np.array(ncfile.variables['Latitude'])\n",
    "    lon  = np.array(ncfile.variables['Longitude'])\n",
    "    attr_lat = ncfile.variables['Latitude']._FillValue\n",
    "    attr_lon = ncfile.variables['Longitude']._FillValue\n",
    "    return lat,lon,CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def countzero(x, axis=1):\n",
    "    #print(x)\n",
    "    count0 = 0\n",
    "    count1 = 0\n",
    "    for i in x:\n",
    "        if i <= 1:\n",
    "            count0 +=1\n",
    "    #print(count0/len(x))\n",
    "    return (count0/len(x))\n",
    "\n",
    "def results(concat_list):\n",
    "    #b2=dd.concat(b1)\n",
    "    #b_2=b2.groupby(['Latitude','Longitude']).mean().reset_index()\n",
    "    #b3=b_2.reset_index()\n",
    "    combs=[]\n",
    "    for x in range(0,180):\n",
    "        for y in range(0,360):\n",
    "            combs.append((x, y))\n",
    "    df_1=pd.DataFrame(combs)\n",
    "    df_1.columns=['Latitude','Longitude']\n",
    "    df4=pd.merge(df_1, df2,on=('Latitude','Longitude'), how='left')\n",
    "    df_cm=df4['CM'].values\n",
    "    np_cm=df_cm.reshape(180,360)\n",
    "    return np_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps:  1 /  ['/Users/dprakas1/Desktop/modis_one/MYD06_L2.A2008001.0005.006.2013341193207.hdf']\n",
      "['/Users/dprakas1/Desktop/modis_one/MYD03.A2008001.0005.006.2012066122516.hdf']\n",
      "['/Users/dprakas1/Desktop/modis_one/MYD06_L2.A2008001.0005.006.2013341193207.hdf']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2030, 1354)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MYD06_dir= '/Users/dprakas1/Desktop/modis_one/'\n",
    "MYD06_prefix = 'MYD06_L2.A2008'\n",
    "MYD03_dir= '/Users/dprakas1/Desktop/modis_one/'\n",
    "MYD03_prefix = 'MYD03.A2008'\n",
    "fileformat = 'hdf'\n",
    "\n",
    "fname1,fname2 = [],[]\n",
    "\n",
    "\n",
    "days = np.arange(1,31,dtype=np.int)\n",
    "for day in days:\n",
    "    dc ='%03i' % day\n",
    "    fname_tmp1 = read_filelist(MYD06_dir,MYD06_prefix,dc,fileformat)\n",
    "    fname_tmp2 = read_filelist(MYD03_dir,MYD03_prefix,dc,fileformat)\n",
    "    fname1 = np.append(fname1,fname_tmp1)\n",
    "    fname2 = np.append(fname2,fname_tmp2)\n",
    "\n",
    "# Initiate the number of day and total cloud fraction\n",
    "files  = np.arange(len(fname1))\n",
    "\n",
    "\n",
    "\n",
    "for j in range(0,1):#hdfs:\n",
    "    print('steps: ',j+1,'/ ',(fname1)) \n",
    "\n",
    "    # Read Level-2 MODIS data\n",
    "    lat,lon,CM = read_MODIS(fname1[j],fname2[j])\n",
    "print((fname2))\n",
    "print((fname1))\n",
    "#rint(CM)\n",
    "#lat = lat.ravel()\n",
    "#lon = lon.ravel()\n",
    "#CM  = CM.ravel()\n",
    "CM.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "combs=[]\n",
    "for x in range(0,180):\n",
    "    for y in range(0,360):\n",
    "        combs.append((x, y))\n",
    "df_1=pd.DataFrame(combs)\n",
    "df_1.columns=['Latitude','Longitude']\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29 µs, sys: 17 µs, total: 46 µs\n",
      "Wall time: 49.8 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "b1=[]\n",
    "def aggregateOneFileData(M06_file, M03_file):\n",
    "    \n",
    "    cm = np.zeros((2030,1354), dtype=np.float32)\n",
    "    lat = np.zeros((2030,1354), dtype=np.float32)\n",
    "    lon = np.zeros((2030,1354), dtype=np.float32)\n",
    "    \n",
    "    #print(fname1,fname2)\n",
    "    myd06 = Dataset(M06_file, \"r\")\n",
    "    CM = myd06.variables[\"Cloud_Mask_1km\"][:,:,0]# Reading Specific Variable 'Cloud_Mask_1km'.\n",
    "    CM = (np.array(CM,dtype='byte') & 0b00000110) >>1\n",
    "    CM = np.array(CM).byteswap().newbyteorder()\n",
    "    \n",
    "    #print(\"CM intial shape:\",CM.shape)\n",
    "    cm = da.concatenate((cm,CM),axis=0)\n",
    "    #print(\"CM shape after con:\",cm.shape)\n",
    "    cm=da.ravel(cm)\n",
    "    #print(\"cm shape after ravel:\",cm.shape)\n",
    "    myd03 = Dataset(M03_file, \"r\")\n",
    "    latitude = myd03.variables[\"Latitude\"][:,:]\n",
    "    longitude = myd03.variables[\"Longitude\"][:,:]\n",
    "    #print(\"Lat intial shape:\",latitude.shape)\n",
    "    #print(\"lon intial shape:\",longitude.shape)\n",
    "    \n",
    "    lat = da.concatenate((lat,latitude),axis=0)\n",
    "    lon = da.concatenate((lon,longitude),axis=0)\n",
    "    #print(\"lat shape after con:\",lat.shape)\n",
    "    #print(\"lon shape after con:\",lon.shape)\n",
    "    \n",
    "    lat=da.ravel(lat)\n",
    "    lon=da.ravel(lon)\n",
    "    \n",
    "    #print(\"lat shape after ravel:\",lat.shape)\n",
    "    #print(\"lon shape after ravel:\",lon.shape)\n",
    "    cm=cm.astype(int)\n",
    "    lon=lon.astype(int)\n",
    "    lat=lat.astype(int)\n",
    "    lat=lat+90\n",
    "    lon=lon+180\n",
    "    Lat=lat.to_dask_dataframe()\n",
    "    Lon=lon.to_dask_dataframe()\n",
    "    CM=cm.to_dask_dataframe()\n",
    "    df=dd.concat([Lat,Lon,CM],axis=1,interleave_partitions=False)\n",
    "    print(type(df))\n",
    "    \n",
    "    cols = {0:'Latitude',1:'Longitude',2:'CM'}\n",
    "    df = df.rename(columns=cols)\n",
    "    \n",
    "    df2=(df.groupby(['Longitude','Latitude']).CM.apply(countzero).reset_index())\n",
    "    df3=df2.compute()\n",
    "    df4=pd.merge(df_1, df3,on=('Latitude','Longitude'), how='left')\n",
    "    df_cm=df4['CM'].values\n",
    "    np_cm=df_cm.reshape(180,360)\n",
    "    return np_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dprakas1/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:49: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n"
     ]
    }
   ],
   "source": [
    "tt=aggregateOneFileData(fname1[0],fname2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       ...,\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dprakas1/anaconda3/lib/python3.7/site-packages/distributed/dashboard/core.py:74: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn(\"\\n\" + msg)\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=client.map(aggregateOneFileData,fname1[0],fname2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Future: status: error, key: aggregateOneFileData-97d3907dc27491a1a4516e7ae69caa0d>,\n",
       " <Future: status: error, key: aggregateOneFileData-ab0c88413c49ca923cdc9fb889b34937>,\n",
       " <Future: status: error, key: aggregateOneFileData-3f1c51c4a70e5549859f394151aa7373>,\n",
       " <Future: status: error, key: aggregateOneFileData-8fef3ce56bcb090b5addc18537b984ab>,\n",
       " <Future: status: error, key: aggregateOneFileData-0cd5ce33a09d03256df02adc2d9a33f0>,\n",
       " <Future: status: error, key: aggregateOneFileData-3f1c51c4a70e5549859f394151aa7373>,\n",
       " <Future: status: error, key: aggregateOneFileData-97d3907dc27491a1a4516e7ae69caa0d>,\n",
       " <Future: status: error, key: aggregateOneFileData-eee3b68f12414c136be29ef7760467ae>,\n",
       " <Future: status: error, key: aggregateOneFileData-4deebe83d5d55f8a0c94122a64e5f0d7>,\n",
       " <Future: status: error, key: aggregateOneFileData-0cd5ce33a09d03256df02adc2d9a33f0>,\n",
       " <Future: status: error, key: aggregateOneFileData-692755c8249735916a13882c2a4fe52a>,\n",
       " <Future: status: error, key: aggregateOneFileData-df67825a91215ecfedcdf160caa3c3cc>,\n",
       " <Future: status: error, key: aggregateOneFileData-692755c8249735916a13882c2a4fe52a>,\n",
       " <Future: status: error, key: aggregateOneFileData-3f1c51c4a70e5549859f394151aa7373>,\n",
       " <Future: status: error, key: aggregateOneFileData-63aa94c14f931973a929c5e77d1d5b34>,\n",
       " <Future: status: error, key: aggregateOneFileData-97d3907dc27491a1a4516e7ae69caa0d>,\n",
       " <Future: status: error, key: aggregateOneFileData-248d8d210163215dafa010d919526ced>,\n",
       " <Future: status: error, key: aggregateOneFileData-8fef3ce56bcb090b5addc18537b984ab>,\n",
       " <Future: status: error, key: aggregateOneFileData-3f1c51c4a70e5549859f394151aa7373>,\n",
       " <Future: status: error, key: aggregateOneFileData-df67825a91215ecfedcdf160caa3c3cc>,\n",
       " <Future: status: error, key: aggregateOneFileData-d77fb538504e172a372c95771a4f4731>,\n",
       " <Future: status: error, key: aggregateOneFileData-168081599685f2581e90c9611af9ae9f>,\n",
       " <Future: status: error, key: aggregateOneFileData-4deebe83d5d55f8a0c94122a64e5f0d7>,\n",
       " <Future: status: error, key: aggregateOneFileData-97d3907dc27491a1a4516e7ae69caa0d>,\n",
       " <Future: status: error, key: aggregateOneFileData-a6fbd31e28c38218a29f26ecc34bf282>,\n",
       " <Future: status: error, key: aggregateOneFileData-168081599685f2581e90c9611af9ae9f>,\n",
       " <Future: status: error, key: aggregateOneFileData-eee3b68f12414c136be29ef7760467ae>,\n",
       " <Future: status: error, key: aggregateOneFileData-2830deb4fa52ffa893ec11c8f6655d16>,\n",
       " <Future: status: error, key: aggregateOneFileData-3f1c51c4a70e5549859f394151aa7373>,\n",
       " <Future: status: error, key: aggregateOneFileData-e9b00f3fffd121a7ee20f24b9471e695>,\n",
       " <Future: status: error, key: aggregateOneFileData-168081599685f2581e90c9611af9ae9f>,\n",
       " <Future: status: error, key: aggregateOneFileData-ccc9404bf6c462ad5a8197a22e8a53a3>,\n",
       " <Future: status: error, key: aggregateOneFileData-8fef3ce56bcb090b5addc18537b984ab>,\n",
       " <Future: status: error, key: aggregateOneFileData-97d3907dc27491a1a4516e7ae69caa0d>,\n",
       " <Future: status: error, key: aggregateOneFileData-967c7be71524f2cdebc82e83418ba57b>,\n",
       " <Future: status: error, key: aggregateOneFileData-ac049fce1ee0b21564efa7f95c0d66f3>,\n",
       " <Future: status: error, key: aggregateOneFileData-248d8d210163215dafa010d919526ced>,\n",
       " <Future: status: error, key: aggregateOneFileData-65e45f8a070da8cf2f900e99fef3fc98>,\n",
       " <Future: status: error, key: aggregateOneFileData-50acaa4c1f88278dfe775ab231babfeb>,\n",
       " <Future: status: error, key: aggregateOneFileData-070281a8a9f2d540d665330d7d043d49>,\n",
       " <Future: status: error, key: aggregateOneFileData-9f42bd01f0532f3bb98a59c16a30b2c7>,\n",
       " <Future: status: error, key: aggregateOneFileData-0f4aceae7a2e634f54e18fbdf61e376b>,\n",
       " <Future: status: error, key: aggregateOneFileData-9af226d05985c3ad39bbc595496aa0ab>,\n",
       " <Future: status: error, key: aggregateOneFileData-33afba0e3cf2bee362822fa1172e1220>,\n",
       " <Future: status: error, key: aggregateOneFileData-3188846cf3ff67648361520243c2bc89>,\n",
       " <Future: status: error, key: aggregateOneFileData-65e45f8a070da8cf2f900e99fef3fc98>,\n",
       " <Future: status: error, key: aggregateOneFileData-65e45f8a070da8cf2f900e99fef3fc98>,\n",
       " <Future: status: error, key: aggregateOneFileData-327a1281d744e97ba471277c68082a27>,\n",
       " <Future: status: error, key: aggregateOneFileData-232b4e71017addb662456ead87e0b9c0>,\n",
       " <Future: status: error, key: aggregateOneFileData-65e45f8a070da8cf2f900e99fef3fc98>,\n",
       " <Future: status: error, key: aggregateOneFileData-d7867acafb2caba21d0c7066b2402a8a>,\n",
       " <Future: status: error, key: aggregateOneFileData-9af226d05985c3ad39bbc595496aa0ab>,\n",
       " <Future: status: error, key: aggregateOneFileData-52925a4a0832c4b63e7d36f2d2125169>,\n",
       " <Future: status: error, key: aggregateOneFileData-232b4e71017addb662456ead87e0b9c0>,\n",
       " <Future: status: error, key: aggregateOneFileData-65e45f8a070da8cf2f900e99fef3fc98>,\n",
       " <Future: status: error, key: aggregateOneFileData-7e0db491098db30ecfbb5eb771ad1d2a>,\n",
       " <Future: status: error, key: aggregateOneFileData-4b56bbc776de92edbaba1c94cb4eefd5>,\n",
       " <Future: status: error, key: aggregateOneFileData-232b4e71017addb662456ead87e0b9c0>,\n",
       " <Future: status: error, key: aggregateOneFileData-742f1ba162e9f01d1fc6a1daf04b689c>,\n",
       " <Future: status: error, key: aggregateOneFileData-cbd9b1fe2c393eb52dfb9f3b50a65e0f>,\n",
       " <Future: status: error, key: aggregateOneFileData-c23b2fd9adac13488065459fc15f821b>,\n",
       " <Future: status: error, key: aggregateOneFileData-0f4aceae7a2e634f54e18fbdf61e376b>,\n",
       " <Future: status: error, key: aggregateOneFileData-65e45f8a070da8cf2f900e99fef3fc98>,\n",
       " <Future: status: error, key: aggregateOneFileData-6c7252f32f353e8dfda070cbcb7c2f3f>,\n",
       " <Future: status: error, key: aggregateOneFileData-8f1dd1083569bb748ca835c10ad90622>,\n",
       " <Future: status: error, key: aggregateOneFileData-2e66b175233c156e5c968c0730a891c3>,\n",
       " <Future: status: error, key: aggregateOneFileData-7c158fd60a50e4b3709726592f4e29ff>,\n",
       " <Future: status: error, key: aggregateOneFileData-a914499aee35821940d7471bba87cde3>,\n",
       " <Future: status: error, key: aggregateOneFileData-b860cad436cfb91cb1a5929e2346f884>,\n",
       " <Future: status: error, key: aggregateOneFileData-6c37398480768b811dce5a9d7b774815>,\n",
       " <Future: status: error, key: aggregateOneFileData-8f1dd1083569bb748ca835c10ad90622>,\n",
       " <Future: status: error, key: aggregateOneFileData-efa68eb7b90f318a1277055ecd7ccc65>,\n",
       " <Future: status: error, key: aggregateOneFileData-bb0fb8f1487a28fc735f2c8e1331ddd8>,\n",
       " <Future: status: error, key: aggregateOneFileData-c3bf9efe141e929593d41ef6c2df36f8>,\n",
       " <Future: status: error, key: aggregateOneFileData-328b355eb398fbc2a71102e6ee6cc94b>]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import dask.delayed as delayed\n",
    "import time\n",
    "import math\n",
    "#import graphviz\n",
    "from netCDF4 import Dataset\n",
    "import os,datetime,sys,fnmatch\n",
    "import h5py\n",
    "import dask\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "M03_dir = \"/Users/dprakas1/Desktop/modis_one/MYD03/\"\n",
    "M06_dir = \"/Users/dprakas1/Desktop/modis_one/MYD06_L2/\"\n",
    "M03_files = sorted(glob.glob(M03_dir + \"MYD03.A2008*\"))\n",
    "M06_files = sorted(glob.glob(M06_dir + \"MYD06_L2.A2008*\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: b'[]'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-8a3990f92e55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyd06\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM06_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mCM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyd06\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Cloud_Mask_1km\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m# Reading Specific Variable 'Cloud_Mask_1km'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mCM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'byte'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m0b00000110\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mCM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyteswap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewbyteorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: b'[]'"
     ]
    }
   ],
   "source": [
    "myd06 = Dataset(M06_files, \"r\")\n",
    "CM = myd06.variables[\"Cloud_Mask_1km\"][:,:,0]# Reading Specific Variable 'Cloud_Mask_1km'.\n",
    "CM = (np.array(CM,dtype='byte') & 0b00000110) >>1\n",
    "CM = np.array(CM).byteswap().newbyteorder()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOD03_path =sys.argv[1] #'HDFFiles/'\n",
    "MOD03_path = '/Users/dprakas1/Desktop/modis_one/'\n",
    "MOD06_path =sys.argv[2] #'HDFFiles/'\n",
    "MOD06_path = '/Users/dprakas1/Desktop/modis_one/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_MODIS_level2_data(MOD06_file,MOD03_file):\n",
    "    #print('Reading The Cloud Mask From MOD06_L2 Product:')\n",
    "    myd06 = Dataset(MOD06_file, \"r\")\n",
    "    CM = myd06.variables[\"Cloud_Mask_1km\"][:,:,:] # Reading Specific Variable 'Cloud_Mask_1km'.\n",
    "    CM   = (np.array(CM[:,:,0],dtype='byte') & 0b00000110) >>1\n",
    "    CM = np.array(CM).byteswap().newbyteorder()\n",
    "   # print('The Level-2 Cloud Mask Array Shape',CM.shape)\n",
    "    print(' ')\n",
    "\n",
    "    myd03 = Dataset(MOD03_file, \"r\")\n",
    "    #print('Reading The Latitude-Longitude From MOD03 Product:')\n",
    "    latitude = myd03.variables[\"Latitude\"][:,:] # Reading Specific Variable 'Latitude'.\n",
    "    latitude = np.array(latitude).byteswap().newbyteorder() # Addressing Byteswap For Big Endian Error.\n",
    "    longitude = myd03.variables[\"Longitude\"][:,:] # Reading Specific Variable 'Longitude'.\n",
    "    longitude = np.array(longitude).byteswap().newbyteorder() # Addressing Byteswap For Big Endian Error.\n",
    "    #print('The Level-2 Latitude-Longitude Array Shape',latitude.shape)\n",
    "    print(' ')\n",
    "\n",
    "    return latitude,longitude,CM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Level 2 GeoLocation & Cloud Data\n",
      " \n",
      " \n",
      "The Number Of Files In The MODO3 List: \n",
      "1\n",
      " \n",
      "The Number Of Files In The MODO6_L2 List: \n",
      "1\n",
      " \n"
     ]
    }
   ],
   "source": [
    "MOD03_filepath = 'MYD03.A*.hdf'\n",
    "MOD06_filepath = 'MYD06_L2.A*.hdf'\n",
    "MOD03_filename, MOD06_filename =[],[]\n",
    "MOD03_filename2, MOD06_filename2 =[],[]\n",
    "\n",
    "for MOD06_filelist in  os.listdir(MOD06_path):\n",
    "    if fnmatch.fnmatch(MOD06_filelist, MOD06_filepath):\n",
    "        MOD06_filename = MOD06_filelist\n",
    "        MOD06_filename2.append(MOD06_filelist)\n",
    "\n",
    "for MOD03_filelist in  os.listdir(MOD03_path):\n",
    "    if fnmatch.fnmatch(MOD03_filelist, MOD03_filepath):\n",
    "        MOD03_filename = MOD03_filelist\n",
    "        MOD03_filename2.append(MOD03_filelist)\n",
    "\n",
    "if MOD03_filename and MOD06_filename:\n",
    "    print('Reading Level 2 GeoLocation & Cloud Data')\n",
    "    Lat,Lon,CM = read_MODIS_level2_data(MOD06_path+MOD06_filename,MOD03_path+MOD03_filename)\n",
    "\n",
    "print('The Number Of Files In The MODO3 List: ')\n",
    "print(len(MOD03_filename2))\n",
    "print(' ')\n",
    "print('The Number Of Files In The MODO6_L2 List: ')\n",
    "print(len(MOD06_filename2))\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MYD03.A2008001.0005.006.2012066122516.hdf']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MOD03_filename2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: b\"['MYD06_L2.A2008001.0005.006.2013341193207.hdf']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-320595260da6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregateOneFileData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMOD06_filename2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMOD03_filename2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36maggregateOneFileData\u001b[0;34m(MOD06_filename2, MOD03_filename2)\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: b\"['MYD06_L2.A2008001.0005.006.2013341193207.hdf']\""
     ]
    }
   ],
   "source": [
    "result=aggregateOneFileData(MOD06_filename2, MOD03_filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "b1=[]\n",
    "def aggregateOneFileData(MOD06_filename2, MOD03_filename2):\n",
    "    \n",
    "    cm = np.zeros((2030,1354), dtype=np.float32)\n",
    "    lat = np.zeros((2030,1354), dtype=np.float32)\n",
    "    lon = np.zeros((2030,1354), dtype=np.float32)\n",
    "    \n",
    "    #print(fname1,fname2)\n",
    "    myd06 = Dataset(MOD06_filename2, \"r\")\n",
    "    CM = myd06.variables[\"Cloud_Mask_1km\"][:,:,0]# Reading Specific Variable 'Cloud_Mask_1km'.\n",
    "    CM = (np.array(CM,dtype='byte') & 0b00000110) >>1\n",
    "    CM = np.array(CM).byteswap().newbyteorder()\n",
    "    \n",
    "    #print(\"CM intial shape:\",CM.shape)\n",
    "    cm = da.concatenate((cm,CM),axis=0)\n",
    "    #print(\"CM shape after con:\",cm.shape)\n",
    "    cm=da.ravel(cm)\n",
    "    #print(\"cm shape after ravel:\",cm.shape)\n",
    "    myd03 = Dataset(MOD03_filename2, \"r\")\n",
    "    latitude = myd03.variables[\"Latitude\"][:,:]\n",
    "    longitude = myd03.variables[\"Longitude\"][:,:]\n",
    "    #print(\"Lat intial shape:\",latitude.shape)\n",
    "    #print(\"lon intial shape:\",longitude.shape)\n",
    "    \n",
    "    lat = da.concatenate((lat,latitude),axis=0)\n",
    "    lon = da.concatenate((lon,longitude),axis=0)\n",
    "    #print(\"lat shape after con:\",lat.shape)\n",
    "    #print(\"lon shape after con:\",lon.shape)\n",
    "    \n",
    "    lat=da.ravel(lat)\n",
    "    lon=da.ravel(lon)\n",
    "    \n",
    "    #print(\"lat shape after ravel:\",lat.shape)\n",
    "    #print(\"lon shape after ravel:\",lon.shape)\n",
    "    cm=cm.astype(int)\n",
    "    lon=lon.astype(int)\n",
    "    lat=lat.astype(int)\n",
    "    lat=lat+90\n",
    "    lon=lon+180\n",
    "    Lat=lat.to_dask_dataframe()\n",
    "    Lon=lon.to_dask_dataframe()\n",
    "    CM=cm.to_dask_dataframe()\n",
    "    df=dd.concat([Lat,Lon,CM],axis=1,interleave_partitions=False)\n",
    "    print(type(df))\n",
    "    \n",
    "    cols = {0:'Latitude',1:'Longitude',2:'CM'}\n",
    "    df = df.rename(columns=cols)\n",
    "    \n",
    "    df2=(df.groupby(['Longitude','Latitude']).CM.apply(countzero).reset_index())\n",
    "    print(type(df2))\n",
    "    combs=[]\n",
    "    for x in range(0,180):\n",
    "        for y in range(0,360):\n",
    "            combs.append((x, y))\n",
    "    df_1=pd.DataFrame(combs)\n",
    "    df_1.columns=['Latitude','Longitude']\n",
    "    df_2=dd.from_pandas(df_1,npartitions=10500)\n",
    "    df4=dd.merge(df_2, b_2,on=('Latitude','Longitude'), how='left')\n",
    "    a = df4['CM'].to_dask_array(lengths=True)\n",
    "    arr = da.asarray(a, chunks=(9257))\n",
    "    final_array=arr.reshape(180,360).compute()\n",
    "    return final_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myd06_name = '/Users/saviosebastian/Documents/Project/CMAC/HDFFiles/'\n",
    "def aggregateOneFileData(M06_file, M03_file):\n",
    "    \n",
    "    cm = np.zeros((2030,1354), dtype=np.float32)\n",
    "    lat = np.zeros((2030,1354), dtype=np.float32)\n",
    "    lon = np.zeros((2030,1354), dtype=np.float32)\n",
    "    \n",
    "    cmfilelist = []\n",
    "\n",
    "    for MOD06_file in MOD06_filename2:\n",
    "        MOD06_file2 = myd06_name + MOD06_file\n",
    "        myd06 = Dataset(MOD06_file2, \"r\")\n",
    "        CM = myd06.variables[\"Cloud_Mask_1km\"][:,:,0]# Reading Specific Variable 'Cloud_Mask_1km'.\n",
    "        CM = (np.array(CM,dtype='byte') & 0b00000110) >>1\n",
    "        CM = np.array(CM).byteswap().newbyteorder()\n",
    "\n",
    "        #----#\n",
    "        cm = da.from_array(CM, chunks =(2030,1354))\n",
    "        #cm = da.from_array(CM, chunks =(500,500))\n",
    "        cmfilelist.append(cm)\n",
    "        cm = da.stack(cmfilelist, axis=0)\n",
    "\n",
    "\n",
    "    print('The Cloud Mask Array Shape Is: ',cm.shape)\n",
    "\n",
    "\n",
    "    #print(\"CM intial shape:\",CM.shape)\n",
    "    cm = da.concatenate((cm,CM),axis=0)\n",
    "    #print(\"CM shape after con:\",cm.shape)\n",
    "    cm=da.ravel(cm)\n",
    "    #print(\"cm shape after ravel:\",cm.shape)\n",
    "    myd03 = Dataset(fname2, \"r\")\n",
    "    latitude = myd03.variables[\"Latitude\"][:,:]\n",
    "    longitude = myd03.variables[\"Longitude\"][:,:]\n",
    "    #print(\"Lat intial shape:\",latitude.shape)\n",
    "    #print(\"lon intial shape:\",longitude.shape)\n",
    "    \n",
    "    lat = da.concatenate((lat,latitude),axis=0)\n",
    "    lon = da.concatenate((lon,longitude),axis=0)\n",
    "    #print(\"lat shape after con:\",lat.shape)\n",
    "    #print(\"lon shape after con:\",lon.shape)\n",
    "    \n",
    "    lat=da.ravel(lat)\n",
    "    lon=da.ravel(lon)\n",
    "    \n",
    "    #print(\"lat shape after ravel:\",lat.shape)\n",
    "    #print(\"lon shape after ravel:\",lon.shape)\n",
    "    cm=cm.astype(int)\n",
    "    lon=lon.astype(int)\n",
    "    lat=lat.astype(int)\n",
    "    lat=lat+90\n",
    "    lon=lon+180\n",
    "    Lat=lat.to_dask_dataframe()\n",
    "    Lon=lon.to_dask_dataframe()\n",
    "    CM=cm.to_dask_dataframe()\n",
    "    df=dd.concat([Lat,Lon,CM],axis=1,interleave_partitions=False)\n",
    "    print(type(df))\n",
    "    \n",
    "    cols = {0:'Latitude',1:'Longitude',2:'CM'}\n",
    "    df = df.rename(columns=cols)\n",
    "    \n",
    "    df2=(df.groupby(['Longitude','Latitude']).CM.apply(countzero).reset_index())\n",
    "    print(type(df2))\n",
    "    combs=[]\n",
    "    for x in range(0,180):\n",
    "        for y in range(0,360):\n",
    "            combs.append((x, y))\n",
    "    df_1=pd.DataFrame(combs)\n",
    "    df_1.columns=['Latitude','Longitude']\n",
    "    df_2=dd.from_pandas(df_1,npartitions=10500)\n",
    "    df4=dd.merge(df_2, b_2,on=('Latitude','Longitude'), how='left')\n",
    "    a = df4['CM'].to_dask_array(lengths=True)\n",
    "    arr = da.asarray(a, chunks=(9257))\n",
    "    final_array=arr.reshape(180,360).compute()\n",
    "    return final_array"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
